import boto3
import os
import mimetypes

# Returns a list of the files changed in the commit
def get_blob_list(codecommit, s3, repository, commit_id):
    response = codecommit.get_commit(repositoryName=repository, commitId=commit_id)

    blob_list = [{'blobId': obj, 'path': ''} for obj in response['commit']['treeId']]

    while 'NextToken' in response:
        response = codecommit.get_commit(repositoryName=repository, commitId=commit_id, nextToken=response['NextToken'])
        blob_list += [{'blobId': obj, 'path': ''} for obj in response['commit']['treeId']]

    for blob in blob_list:
        obj = s3.get_object(Bucket=os.environ['s3BucketName'], Key=blob['blobId'])
        blob['path'] = obj['Metadata'].get('path', '')

    return blob_list



# Lambda function triggered by changes in a CodeCommit repository
# Reads files in the repository and uploads them to an S3 bucket
#
# ENVIRONMENT VARIABLES:
#     s3BucketName
#     s3FolderPath
#     codecommitRegion
#     repository
#     branch
#
# TIME OUT: 1 min
#
# EXECUTION ROLE
#     lambda-codecommit-s3-execution-role (permissions: AWSCodeCommitReadOnly, AWSLambdaExecute)
def lambda_handler(event, context):
    # Get the S3 bucket, CodeCommit, and S3 client
    s3 = boto3.client('s3')
    bucket = boto3.resource('s3').Bucket(os.environ['s3BucketName'])
    codecommit = boto3.client('codecommit', region_name=os.environ['codecommitRegion'])
    
    # Get the last commit id from the CodeCommit event
    last_commit_id = event['Records'][0]['codecommit']['references'][0]['commit']
    print(f"Last commit id: {last_commit_id}")
    
    # Get the list of files changed in the last commit
    blob_list = get_blob_list(codecommit, s3, os.environ['repository'], last_commit_id)
    print(f"Files to upload: {blob_list}")
    
    # Upload each file to the specified S3 folder
    for blob in blob_list:
        print(f"Blob: {blob}")
        path = os.path.join(os.environ['s3FolderPath'], blob['path'])
        content = s3.get_object(Bucket=os.environ['s3BucketName'], Key=blob['blobId'])['Body'].read()

        # Guess the mime content-type of the files and provide it to S3 since S3 cannot do this on its own
        content_type = mimetypes.guess_type(path)[0]
        if content_type is not None:
            bucket.put_object(Body=content, Key=path, ContentType=content_type)
        else:
            bucket.put_object(Body=content, Key=path)
            
    print("Files uploaded successfully to S3.")
